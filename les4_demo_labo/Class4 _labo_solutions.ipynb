{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning- tree-based methods + whitening the blackbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "df = pd.read_csv('real_estate_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Taking a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train/test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "from random import Random\n",
    "df.iloc[:,25:39] = df.iloc[:,25:39].astype(np.uint8)\n",
    "\n",
    "df_shuffle = df.sample(frac=1, random_state=123)\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_shuffle.drop(['tx_price'],1)\n",
    "y = df_shuffle['tx_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# standardise\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_feat = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[num_feat])\n",
    "\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "X_train_stand[num_feat] = scaler.transform(X_train[num_feat])\n",
    "X_test_stand[num_feat] = scaler.transform(X_test[num_feat])\n",
    "\n",
    "X_test_stand.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Perform a crossvalidation, using DecisionTreeRegressor, with depth ranging from 1 to 50\n",
    "  \n",
    "2. Plot the depth to the R2\n",
    "    + First do a general plot\n",
    "    + Then zoom in on the highest values \n",
    "    + Which depth leads to the highest R2?\n",
    "    + How do you explain the horizontal line on the right side of the general plot? Or in other words: how come the R2 remains constant as the depth increases?\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "depth = np.arange(1,50)    # This will give an array of numbers between 1 and 10\n",
    "cv_scores = []\n",
    "sd_scores = []\n",
    "# perform 5-fold cross validation on the  possible values for the radius (bandwith)\n",
    "for d in depth:\n",
    "    dec_tree = DecisionTreeRegressor(random_state = 0, max_depth=d)  \n",
    "    scores = cross_val_score(dec_tree, X_train_stand, y_train,  cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "    sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = max(cv_scores)\n",
    "max_index = cv_scores.index(max_value)\n",
    "\n",
    "plt.plot(depth, cv_scores)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('R^2')\n",
    "\n",
    "plt.show()\n",
    "print('The best depth is', depth[max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = max(cv_scores)\n",
    "max_index = cv_scores.index(max_value)\n",
    "\n",
    "plt.plot(depth[2:11], cv_scores[2:11])\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('R^2')\n",
    "\n",
    "plt.show()\n",
    "print('The best depth is', depth[max_index])\n",
    "print('The best validation scoer is', max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A depth of 5 gives the best model\n",
    "+ The more simple model with a dept of 4 is above the lower boundary, so this is the optimal model.\n",
    "+ The horizontal line can be explained by the fact the maximum depth of a decision tree is limited. We can see that at around a depth of 21 all observations are in a leaf of their own, so no more splitting is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Retrain and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree  \n",
    "dec_tree = DecisionTreeRegressor(random_state = 0, max_depth=4)  \n",
    "dec_tree.fit(X_train_stand, y_train) \n",
    "print(dec_tree.score(X_train_stand, y_train) )\n",
    "dec_tree.score(X_test_stand, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already better than any of the model we have tried before. And we have just used the most simple tree-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random forest\n",
    "Let's see if we can improve the previous result by using random forests. \n",
    "\n",
    "1. Perform a  random search cross-validation using  RandomForestRegressor with the following hyperparameters\n",
    "    + criterion: mse, mae\n",
    "    + n_estimators: choose 200 values, ranging from 100 to 500\n",
    "    + max_features: auto, sqrt, log2\n",
    "    + max_depth: let it range from 1 to 15\n",
    "    + min_samples_split: let it range from 2 to 15: why not from 1 to 15?\n",
    "    + min_samples_leaf: let it range from 1 to 15\n",
    "    + max_leaf_nodes: choose 490 values, ranging from 10 to 500\n",
    "    \n",
    "    + use random_state=42\n",
    "    + use 3 folds\n",
    "    + fit 100 random models\n",
    "    \n",
    "2. Print out the validated $R^2$ for the best model from the random search \n",
    " \n",
    "3. Print out the parameters of the best model\n",
    " \n",
    "4. Plot the results for all hyperparameters\n",
    "     + First make a general plot for all results and all hyperparameters\n",
    "     + Then make plots for all hyperparameters, but only the models where the R2 is above 0.7, to get a better idea which parameters you will use in the grid search.\n",
    "     \n",
    "5. Perform a grid search. Choose the parameters that you include in this search yourself\n",
    "      + Since this is just a labo and you do not want to wait hours for the results, limit the choices. For an assignment, you should add more options.\n",
    "      + print out the best model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "criterion =['mse','mae']\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 200)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(1, 15, num = 15)]\n",
    "min_samples_split = [int(x) for x in np.linspace(2, 15, num = 14)]\n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 15, num = 15)]\n",
    "max_leaf_nodes = [int(x) for x in np.linspace(10, 500, num = 490)]\n",
    "\n",
    "# create the random grid to search for best hyperparameters\n",
    "random_grid = {'criterion': criterion,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "# then do cross-validatoin\n",
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, \n",
    "                               cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "# n_jobs=-1 to run as many models  parallel as possible\n",
    "rf_random.fit(X_train_stand, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print out the training and validated $R^2$ for the best model from the random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_random.score(X_train_stand, y_train))\n",
    "print(rf_random.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. print out the parameters of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the results  for the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(rf_random.cv_results_)\n",
    "\n",
    "xlabel_names = ['param_max_depth','param_min_samples_split','param_min_samples_leaf','param_n_estimators',\n",
    "                'param_max_features', 'param_max_leaf_nodes', 'param_criterion']\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "axs[0,0].scatter(out['param_max_depth'], out['mean_test_score'], c='blue');\n",
    "axs[0,0].set_title('max_depth')\n",
    "\n",
    "axs[0,1].scatter(out['param_min_samples_split'], out['mean_test_score'], c='blue');\n",
    "axs[0,1].set_title('min_samples_split')\n",
    "\n",
    "axs[0,2].scatter(out['param_min_samples_leaf'], out['mean_test_score'], c='blue');\n",
    "axs[0,2].set_title('min_samples_leaf')\n",
    "\n",
    "axs[1,0].scatter(out['param_n_estimators'], out['mean_test_score'], c='blue');\n",
    "axs[1,0].set_title('n_estimators')\n",
    "\n",
    "axs[1,1].scatter(out['param_max_features'], out['mean_test_score'], c='blue');\n",
    "axs[1,1].set_title('max_features')\n",
    "\n",
    "axs[1,2].scatter(out['param_max_leaf_nodes'], out['mean_test_score'], c='blue');\n",
    "axs[1,2].set_title('max_leaf_nodes')\n",
    "\n",
    "axs[2,0].scatter(out['param_criterion'], out['mean_test_score'], c='blue');\n",
    "axs[2,0].set_title('criterion')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(ylabel='r_squared')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out[out.mean_test_score > 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_max_depth'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"max_depth\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best depth is 8, but 7 and 9 do not perform much worse. However, we prefer models that are less complex. Here, that means models that are less deep. So, we chose 7 and 8 to go in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_min_samples_split'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"min_samples_split\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best value is 2, but 4 does not perform much worse. However, we prefer models that are less complex. Here, that means models that have a higher min number of samples to allow splitting. So, we chose 2 and 4 to go in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_min_samples_leaf'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"min_samples_leaf\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 leads to the best R^2, but 5 is not that much worse. We prefer a higher number of samples in a leaf (less complex), thus we include 3 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_n_estimators'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"n_estimators\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value was with 234 trees, but around 230 does not perform much worse. Let's add 230,232 and 234 to the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_max_features'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"max_features\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto clearly wins here, so let's just go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_max_leaf_nodes'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"max_leaf_nodes\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best max_leaf_nodes was 60, but around 290 is not much worse. Let's add 60 and 290 to the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_criterion'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"criterion\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mse had the best score,we use mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "criterion =[ 'mse']\n",
    "n_estimators = [230, 232,234]\n",
    "max_features = ['auto']\n",
    "max_depth = [7,8]\n",
    "min_samples_split = [2,4]\n",
    "min_samples_leaf = [3,5]\n",
    "max_leaf_nodes = [60, 290]\n",
    "\n",
    "\n",
    "# create the random grid to search for best hyperparameters\n",
    "grid = {'criterion': criterion,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# then do cross-validatoin\n",
    "rf = RandomForestRegressor()\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = grid,\n",
    "                               cv = 5, verbose=2,  n_jobs=-1)\n",
    "# n_jobs=-1 to run as many models  parallel as possible\n",
    "rf_grid.fit(X_train_stand, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a lot has changed, this is not that unexpected, because we did not give a lot of options in the grid search. I would try a little harder for an assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('R2: %.3f' % rf_grid.score(X_train_stand, y_train))\n",
    "print('R2: %.3f' % rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest gives us the best model untill now. (compared to linear regression, lasso regression and random forest). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Whitening the black box\n",
    "\n",
    "Untill know we have focussed on getting a model that performs as best as possible. But let's shift our attention now. How do we interpret these models?\n",
    "1. Which features are important: Plot the feature importances\n",
    "    + Which five features are most important?\n",
    "2. What is the influence of a certain feature?\n",
    "    + Get the predictions from the random forest.\n",
    "    + Perform one univariate linear regressions per feature on the predicted values!\n",
    "        + Use the standardized features and also standardize the predictions, to make the interpretation easier \n",
    "    + Look at the direction of the effect for the five most important features\n",
    "3. Why is a prediction what it is?\t\n",
    "    + Use lime to get an idea of how the prediction of observation 100 was made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "feat_importances = pd.Series(rf_gridBest.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(38).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the five features that seem to have the most influence in our algorithm are the cost for insuring the house, the year the taxation is made, the hight of the property tax, the year built and the square feet.\n",
    "\n",
    "But what is the direction of this influence? Here, you might guess it from the context ( a house with a higher insurance cost, will probably have a higher price), but be carefull with making assumptions like that. There is always a chance that you are wrong. So you have to check the directions of the influences yourself.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the influence of a certain feature?\u000b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ get the predictions from the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train =rf_gridBest.predict(X_train_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Perform one univariate linear regressions per feature on the predicted values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = pd.DataFrame(X_train_stand)\n",
    "X.columns = X_train.columns\n",
    "predictors =  X_train.columns\n",
    "y = np.array((pred_train-pred_train.mean())/np.sqrt(pred_train.var())).reshape(-1, 1)\n",
    "\n",
    "reg = LinearRegression(normalize=True).fit(X[[predictors[0]]], y)\n",
    "beta = pd.Series(reg.coef_[0])\n",
    "names = pd.Series(predictors[0])\n",
    "for i in np.arange(1,(X.shape[1])):\n",
    "    reg = LinearRegression(normalize=True).fit(X[[predictors[i]]], y)\n",
    "    beta_help = pd.Series(reg.coef_[0])\n",
    "    names_help = pd.Series(predictors[i])\n",
    "    beta = pd.concat([beta,beta_help], axis=0)\n",
    "    names = pd.concat([names,names_help], axis=0)\n",
    "betas = pd.concat([names,beta],axis=1)\n",
    "betas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the higher the sqft, the higher the price;  the higher the property tax, the higher the price;...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is a prediction what it is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "\n",
    "# creating the explainer function\n",
    "explainer = LimeTabularExplainer(X_train_stand.values, mode=\"regression\", feature_names=X_train.columns)\n",
    "\n",
    "# storing a new observation\n",
    "i = 100\n",
    "X_test_stand = pd.DataFrame(X_test_stand)\n",
    "X_test_stand.columns = X_train.columns\n",
    "\n",
    "X_observation = X_test_stand.iloc[[i], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation using the random forest model\n",
    "explanation = explainer.explain_instance(X_observation.values[0], rf_gridBest.predict)\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the prediction for this point is 12.93 (middel of the left bar). By permuting the values of the different features, the prediction ranged from 12.33 to 13.49. In the table on the right, we can see the point itself. In the graph in the middle, you can see how the features influence the point. So, if the insurance is above 0;55, this will have a positive impact on the prediction (the prediction will be higher)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gradient boosting\n",
    "Let's try one last algorithm for regression. \n",
    "\n",
    "1. Perform a  random search cross-validation using  GradientBoostingRegressor with the following hyperparameters\n",
    "    + criterion: mse, mae\n",
    "    + n_estimators: choose 50 values, ranging from 100 to 500\n",
    "    + max_features: auto, sqrt, log2\n",
    "    + max_depth: let it range from 1 to 15\n",
    "    + min_samples_split: let it range from 2 to 15: why not from 1 to 15?\n",
    "    + min_samples_leaf: let it range from 1 to 15\n",
    "    + max_leaf_nodes: choose 50 values, ranging from 10 to 500\n",
    "    + loss: ls, lad, huber, quantile\n",
    "    + learning_rate = choose 10 values, ranging from 0.01 to 1\n",
    "    \n",
    "    + use random_state=42\n",
    "    + use 3 folds\n",
    "    + fit 100 random models\n",
    "    \n",
    "2. Print out the validated $R^2$ for the best model from the random search \n",
    "\n",
    "3. Print out the parameters of the best model\n",
    " \n",
    "4. Plot the results for all hyperparameters\n",
    "     + First make a general plot for all results and all hyperparameters\n",
    "     + Then make plots for all hyperparameters, but only the models where the R2 is above 0.7, to get a better idea which parameters you will use in the grid search.\n",
    "     \n",
    "5. Perform a grid search. Choose the parameters that you include in this search yourself\n",
    "      + Since this is just a labo and you do not want to wait hours for the results, limit the choices. For an assignment, you could add more options.\n",
    "      + print out the best model\n",
    "      + Test the model \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Perform random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "criterion =['mse','mae']\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 50)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(1, 15, num = 15)]\n",
    "min_samples_split = [int(x) for x in np.linspace(2, 15, num = 14)]\n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 15, num = 15)]\n",
    "max_leaf_nodes = [int(x) for x in np.linspace(10, 500, num = 50)]\n",
    "loss = ['ls','lad','huber','quantile']\n",
    "learning_rate = [round(x,5) for x in np.linspace(0.1, 1, num = 10)]\n",
    "\n",
    "# create the random grid to search for best hyperparameters\n",
    "random_grid = {'criterion': criterion,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'max_leaf_nodes': max_leaf_nodes,\n",
    "               'loss': loss,\n",
    "               'learning_rate': learning_rate}\n",
    "\n",
    "# then do cross-validation\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_random = RandomizedSearchCV(estimator = gbm, param_distributions = random_grid, n_iter = 100, \n",
    "                               cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "X_train_stand.info()\n",
    "#gbm_random.fit(X_train_stand, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print out the validated $R^2$ for the best model from the random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbm_random.score(X_train_stand, y_train))\n",
    "gbm_random.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. print out the parameters of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the results of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(gbm_random.cv_results_)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = out\n",
    "\n",
    "xlabel_names = ['param_max_depth','param_min_samples_split','param_min_samples_leaf','param_n_estimators',\n",
    "                'param_max_features', 'param_max_leaf_nodes', 'param_criterion']\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "axs[0,0].scatter(out2['param_max_depth'], out2['mean_test_score'], c='blue');\n",
    "axs[0,0].set_title('max_depth')\n",
    "\n",
    "axs[0,1].scatter(out2['param_min_samples_split'], out2['mean_test_score'], c='blue');\n",
    "axs[0,1].set_title('min_samples_split')\n",
    "\n",
    "axs[0,2].scatter(out2['param_min_samples_leaf'], out2['mean_test_score'], c='blue');\n",
    "axs[0,2].set_title('min_samples_leaf')\n",
    "\n",
    "axs[1,0].scatter(out2['param_n_estimators'], out2['mean_test_score'], c='blue');\n",
    "axs[1,0].set_title('n_estimators')\n",
    "\n",
    "axs[1,1].scatter(out2['param_max_features'], out2['mean_test_score'], c='blue');\n",
    "axs[1,1].set_title('max_features')\n",
    "\n",
    "axs[1,2].scatter(out2['param_max_leaf_nodes'], out2['mean_test_score'], c='blue');\n",
    "axs[1,2].set_title('max_leaf_nodes')\n",
    "\n",
    "axs[2,0].scatter(out2['param_criterion'], out2['mean_test_score'], c='blue');\n",
    "axs[2,0].set_title('criterion')\n",
    "\n",
    "axs[2,1].scatter(out2['param_learning_rate'], out2['mean_test_score'], c='blue');\n",
    "axs[2,1].set_title('learning_rate')\n",
    "\n",
    "axs[2,2].scatter(out2['param_loss'], out2['mean_test_score'], c='blue');\n",
    "axs[2,2].set_title('loss')\n",
    "\n",
    "\n",
    "for ax in axs.flat: ax.set(ylabel='r_squared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to see anything on these graphs, because of some very low scores. We will zoom in on the R^2 above 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = out[out.mean_test_score > 0.7]\n",
    "\n",
    "\n",
    "xlabel_names = ['param_max_depth','param_min_samples_split','param_min_samples_leaf','param_n_estimators',\n",
    "                'param_max_features', 'param_max_leaf_nodes', 'param_criterion']\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "axs[0,0].scatter(out2['param_max_depth'], out2['mean_test_score'], c='blue');\n",
    "axs[0,0].set_title('max_depth')\n",
    "\n",
    "axs[0,1].scatter(out2['param_min_samples_split'], out2['mean_test_score'], c='blue');\n",
    "axs[0,1].set_title('min_samples_split')\n",
    "\n",
    "axs[0,2].scatter(out2['param_min_samples_leaf'], out2['mean_test_score'], c='blue');\n",
    "axs[0,2].set_title('min_samples_leaf')\n",
    "\n",
    "axs[1,0].scatter(out2['param_n_estimators'], out2['mean_test_score'], c='blue');\n",
    "axs[1,0].set_title('n_estimators')\n",
    "\n",
    "axs[1,1].scatter(out2['param_max_features'], out2['mean_test_score'], c='blue');\n",
    "axs[1,1].set_title('max_features')\n",
    "\n",
    "axs[1,2].scatter(out2['param_max_leaf_nodes'], out2['mean_test_score'], c='blue');\n",
    "axs[1,2].set_title('max_leaf_nodes')\n",
    "\n",
    "axs[2,0].scatter(out2['param_criterion'], out2['mean_test_score'], c='blue');\n",
    "axs[2,0].set_title('criterion')\n",
    "\n",
    "axs[2,1].scatter(out2['param_learning_rate'], out2['mean_test_score'], c='blue');\n",
    "axs[2,1].set_title('learning_rate')\n",
    "\n",
    "axs[2,2].scatter(out2['param_loss'], out2['mean_test_score'], c='blue');\n",
    "axs[2,2].set_title('loss')\n",
    "\n",
    "\n",
    "for ax in axs.flat: ax.set(ylabel='r_squared')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "criterion =['mse']\n",
    "n_estimators = [214, 251]\n",
    "max_features = ['auto']\n",
    "max_depth = [3,5]\n",
    "min_samples_split = [9]\n",
    "min_samples_leaf = [7,9]\n",
    "max_leaf_nodes = [198,410]\n",
    "learning_rate = [0.09,0.1,0.11,0.12,0.13]\n",
    "loss = ['lad']\n",
    "\n",
    "# create the random grid to search for best hyperparameters\n",
    "grid = {'criterion': criterion,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "            'max_leaf_nodes': max_leaf_nodes,\n",
    "               'learning_rate': learning_rate,\n",
    "               'loss':loss}\n",
    "\n",
    "# then do cross-validatoin\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_grid = GridSearchCV(estimator = gbm, param_grid = grid,\n",
    "                               cv = 5, verbose=2,  n_jobs=-1)\n",
    "# n_jobs=-1 to run as many models  parallel as possible\n",
    "gbm_grid.fit(X_train_stand, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbm_random.score(X_train_stand, y_train))\n",
    "gbm_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final evaluation\n",
    "+ Choose the model with the best validated score from this labo\n",
    "+ Retrain the model on the whole training set\n",
    "+ Evaluate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOw that we have compared all algorithms, we choose the algorithm with the highest validated score. Retrain this algorithm on the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = gbm_grid.best_params_\n",
    "gbm_gridBest = GradientBoostingRegressor(**params)\n",
    "gbm_gridBest.fit(X_train, y_train)\n",
    "print(gbm_gridBest.score(X_train, y_train))\n",
    "print(gbm_gridBest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has a reliability of 81%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(0.01,1,10) # logaritmische schaal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.01,1,10) # lineair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "les3_demo_labo-LdZrQQAt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "713bfb1b668227f3a47885ca66d7d7efa48fac74882665024e464ea8e31189a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
